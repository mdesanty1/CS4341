{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir images\n",
        "!wget https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
        "!tar zxvf flower_photos.tgz -C /content/images/"
      ],
      "metadata": {
        "id": "q4ZvGci9t68u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "###########################MAGIC HAPPENS HERE##########################\n",
        "# Change the hyper-parameters to get the model performs well\n",
        "config = {\n",
        "    'batch_size': 64,\n",
        "    'image_size': (30,30),\n",
        "    'epochs': 20,\n",
        "    'optimizer': keras.optimizers.experimental.SGD(1e-2)\n",
        "}\n",
        "###########################MAGIC ENDS  HERE##########################\n",
        "\n",
        "def read_data():\n",
        "    train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        \"./images/flower_photos\",\n",
        "        validation_split=0.2,\n",
        "        subset=\"both\",\n",
        "        seed=42,\n",
        "        image_size=config['image_size'],\n",
        "        batch_size=config['batch_size'],\n",
        "        labels='inferred',\n",
        "        label_mode = 'int'\n",
        "    )\n",
        "    val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "    test_ds = val_ds.take(val_batches // 2)\n",
        "    val_ds = val_ds.skip(val_batches // 2)\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "def data_processing(ds):\n",
        "    data_augmentation = keras.Sequential(\n",
        "        [\n",
        "            ###########################MAGIC HAPPENS HERE##########################\n",
        "            # Use dataset augmentation methods to prevent overfitting,\n",
        "            layers.RandomFlip(\"horizontal\"),\n",
        "            layers.RandomRotation(0.3)\n",
        "            ###########################MAGIC ENDS HERE##########################\n",
        "        ]\n",
        "    )\n",
        "    ds = ds.map(\n",
        "        lambda img, label: (data_augmentation(img), label),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "    ###########################MAGIC HAPPENS HERE##########################\n",
        "    # Build up a neural network to achieve better performance.\n",
        "    # Use Keras API like `x = layers.XXX()(x)`\n",
        "    # Hint: Use a Deeper network (i.e., more hidden layers, different type of layers)\n",
        "    # and different combination of activation function to achieve better result.\n",
        "    hidden_units = 4\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(hidden_units, activation='relu')(x)\n",
        "\n",
        "    ###########################MAGIC ENDS HERE##########################\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\", kernel_initializer='he_normal')(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load and Process the dataset\n",
        "    train_ds, val_ds, test_ds = read_data()\n",
        "    train_ds = data_processing(train_ds)\n",
        "    # Build up the ANN model\n",
        "    model = build_model(config['image_size']+(3,), 5)\n",
        "    # Compile the model with optimizer and loss function\n",
        "    model.compile(\n",
        "        optimizer=config['optimizer'],\n",
        "        loss='SparseCategoricalCrossentropy',\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    # Fit the model with training dataset\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=config['epochs'],\n",
        "        validation_data=val_ds\n",
        "    )\n",
        "    ###########################MAGIC HAPPENS HERE##########################\n",
        "    print(history.history)\n",
        "    test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
        "    print(\"\\nTest Accuracy: \", test_acc)\n",
        "    test_images = np.concatenate([x for x, y in test_ds], axis=0)\n",
        "    test_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "    test_prediction = np.argmax(model.predict(test_images),1)\n",
        "    # 1. Visualize the confusion matrix by matplotlib and sklearn based on test_prediction and test_labels\n",
        "    # 2. Report the precision and recall for 10 different classes\n",
        "    # Hint: check the precision and recall functions from sklearn package or you can implement these function by yourselves.\n",
        "    # 3. Visualize three misclassified images\n",
        "    # Hint: Use the test_images array to generate the misclassified images using matplotlib\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ###########################MAGIC HAPPENS HERE##########################"
      ],
      "metadata": {
        "id": "j_d7I1krgvo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bD5SxuaO05RN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}